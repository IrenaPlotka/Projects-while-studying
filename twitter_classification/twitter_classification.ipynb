{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Classification Project\n",
    "\n",
    "The aim of the project is to find patterns in real tweets. The files new_york.json, london.json, and paris.json contain tweets that were gathered from those locations. The goal is to create a classification algorithm that can classify any tweet (or sentence) and predict whether that sentence came from New York, London, or Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4723\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',\n",
      "       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'quote_count', 'reply_count',\n",
      "       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',\n",
      "       'filter_level', 'lang', 'timestamp_ms', 'extended_tweet',\n",
      "       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',\n",
      "       'quoted_status', 'quoted_status_permalink', 'extended_entities',\n",
      "       'withheld_in_countries'],\n",
      "      dtype='object')\n",
      "texting me bullshit i just swipe and delete it\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_york_tweets = pd.read_json('https://raw.githubusercontent.com/IrenaPlotka/codecademy_projects/master/twitter_classification/new_york.json', lines=True)\n",
    "# print(new_york_tweets.head())\n",
    "print(len(new_york_tweets))\n",
    "print(new_york_tweets.columns)\n",
    "print(new_york_tweets.loc[5]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5341\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',\n",
      "       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'extended_tweet', 'quote_count',\n",
      "       'reply_count', 'retweet_count', 'favorite_count', 'entities',\n",
      "       'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms',\n",
      "       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',\n",
      "       'quoted_status', 'quoted_status_permalink', 'extended_entities'],\n",
      "      dtype='object')\n",
      "Whatâ€™s cooler than being cool? \n",
      " - Ice Cold matcha latte - Simple n delish ðŸŒ´ðŸŒ´\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "#matcha #matchateaâ€¦ https://t.co/jnDjiyimov\n"
     ]
    }
   ],
   "source": [
    "london_tweets = pd.read_json('https://raw.githubusercontent.com/IrenaPlotka/codecademy_projects/master/twitter_classification/london.json', lines=True)\n",
    "print(len(london_tweets))\n",
    "print(london_tweets.columns)\n",
    "print(london_tweets.loc[5]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2510\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'source', 'truncated',\n",
      "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'quote_count', 'reply_count',\n",
      "       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',\n",
      "       'filter_level', 'lang', 'timestamp_ms', 'display_text_range',\n",
      "       'extended_entities', 'possibly_sensitive', 'quoted_status_id',\n",
      "       'quoted_status_id_str', 'quoted_status', 'quoted_status_permalink',\n",
      "       'extended_tweet'],\n",
      "      dtype='object')\n",
      "Finally, rain in Paris. #aurevoirlahaut @ Paris, France https://t.co/9x3oUakKj1\n"
     ]
    }
   ],
   "source": [
    "paris_tweets = pd.read_json('https://raw.githubusercontent.com/IrenaPlotka/codecademy_projects/master/twitter_classification/paris.json', lines=True)\n",
    "print(len(paris_tweets))\n",
    "print(paris_tweets.columns)\n",
    "print(paris_tweets.loc[5]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first uniting all the texts of tweets in one list\n",
    "new_york_text = new_york_tweets['text'].tolist()\n",
    "london_text = london_tweets['text'].tolist()\n",
    "paris_text = paris_tweets['text'].tolist()\n",
    "all_tweets = new_york_text + london_text + paris_text\n",
    "\n",
    "# making the labels associated with those tweets: 0 - a New York tweet, 1 - a Lodon tweet, 2 - a Paris tweet\n",
    "lables = [0]*len(new_york_text) + [1]*len(london_text) + [2]*len(paris_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10059\n",
      "2515\n"
     ]
    }
   ],
   "source": [
    "# breaking the data into a training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(all_tweets, lables, test_size = 0.2, random_state = 1)\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another day in #London #UK #ðŸ‡¬ðŸ‡§ å ´æ‰€: Covent Garden London https://t.co/M2T85Hc1Z2\n",
      "  (0, 2625)\t1\n",
      "  (0, 6215)\t1\n",
      "  (0, 6855)\t1\n",
      "  (0, 7507)\t1\n",
      "  (0, 11213)\t1\n",
      "  (0, 13029)\t1\n",
      "  (0, 13530)\t1\n",
      "  (0, 16414)\t2\n",
      "  (0, 16766)\t1\n",
      "  (0, 27960)\t1\n",
      "  (0, 32472)\t1\n"
     ]
    }
   ],
   "source": [
    "# transforming lists of words into count vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "counter = CountVectorizer()\n",
    "counter.fit(train_data)\n",
    "train_counts = counter.transform(train_data)\n",
    "test_counts = counter.transform(test_data)\n",
    "# checking how a tweet looks as a Count Vector\n",
    "print(train_data[102])\n",
    "print(train_counts[102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Count Vectors to train and test Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_counts, train_labels)\n",
    "predictions = classifier.predict(test_counts)\n",
    "# print(classifier.score(test_counts, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6779324055666004\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model with accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[587 329  57]\n",
      " [249 742  70]\n",
      " [ 53  70 358]]\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model with confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows that 587 tweets from NY were associated correctly, 329 were associated as tweets from London and 57 from Paris. 742 tweets from London and 358 tweets from Paris were labeled right. As it turns out tweets coming from two English speaking countries are harder to distinguish than tweets in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# testing my own tweets\n",
    "tweet = 'seems like me and my neighbors have the same music taste #someone #likes #loudmusic'\n",
    "tweet_counts = counter.transform([tweet])\n",
    "print(classifier.predict(tweet_counts))\n",
    "tweet2 = 'the weather drives me crazy, how much longer will it last #rainingagain'\n",
    "tweet_counts2 = counter.transform([tweet2])\n",
    "print(classifier.predict(tweet_counts2))\n",
    "tweet3 = 'rien ne peut Ãªtre mieux que des vacances en montagne'\n",
    "tweet_counts3 = counter.transform([tweet3])\n",
    "print(classifier.predict(tweet_counts3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the 3 tweets were labeled correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
